{
  "name": "openai-ollama-local-proxy",
  "version": "1.0.0",
  "description": "A proxy server that redirects OpenAI API requests to a local Ollama instance",
  "main": "src/index.js",
  "type": "module",
  "scripts": {
    "start": "sudo node src/index.js",
    "dev": "nodemon src/index.js",
    "test": "node --experimental-vm-modules node_modules/.bin/jest"
  },
  "keywords": [
    "openai",
    "ollama",
    "proxy",
    "api",
    "llm"
  ],
  "author": "",
  "license": "MIT",
  "dependencies": {
    "axios": "^1.6.2",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "morgan": "^1.10.0",
    "node-mocks-http": "^1.17.2"
  },
  "devDependencies": {
    "jest": "^29.0.0",
    "nodemon": "^3.0.1",
    "supertest": "^6.3.0"
  }
}
